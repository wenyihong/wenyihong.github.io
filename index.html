<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Wenyi Hong</title>

  <meta name="author" content="Wenyi Hong">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">

</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p class="name" style="text-align: center;">
                    Wenyi Hong
                  </p>
                  <p>
                    I am a third-year PhD student in Computer Science at Tsinghua University since 2022, supervised by
                    Prof. <a href="http://keg.cs.tsinghua.edu.cn/jietang/">Jie Tang</a>. Before that, I received my Bachelorâ€™s degree in Computer Science and Technology at Tsinghua University with the GPA of 4.00/4.00 (rank 1/238).
                  </p>
                  <p>
                    My research primarily focuses on multimodal foundation models, including vision-language models, multimodal agents, and
                    vision generation models.
                  </p>
                  <p style="text-align:center">
                    <a href="mailto:wenyi.hong@outlook.com">Email</a> &nbsp;/&nbsp;
                    <!-- <a href="data/JonBarron-CV.pdf">CV</a> &nbsp;/&nbsp; -->
                    <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp;/&nbsp; -->
                    <a href="https://scholar.google.com/citations?user=JSEzrlwAAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
                    <!-- <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp;/&nbsp; -->
                    <!-- <a href="https://bsky.app/profile/jonbarron.bsky.social">Bluesky</a> &nbsp;/&nbsp; -->
                    <a href="https://github.com/wenyihong/">Github</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:37%;max-width:37%">
                  <a href="images/WenyiHong.jpg"><img
                      style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo"
                      src="images/WenyiHong.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>


          <!-- <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:16px;width:100%;vertical-align:middle">
                  <h2>Selected Publications</h2>
                  <p>
                    (* indicates equal contribution).
                  </p>
                </td>
              </tr>
            </tbody>
          </table> -->

          <section class="honors-section">
            <h2>Honors & Awards</h2>
            <ul class="awards-list">
              <li><span class="year">2024:</span> <strong>CVPR 2024 Highlight Paper</strong> for CogAgent</li>
              <li><span class="year">2024:</span> <strong>ICLR 2024 Spotlight Paper</strong> for RelayDiffusion</li>
              <li><span class="year">2023:</span> Selected to Tsinghua University's Future Scholars Scholarship Program</li>
              <li><span class="year">2022:</span> Outstanding Graduate of Tsinghua University</li>
              <li><span class="year">2022:</span> Outstanding Graduate of Beijing</li>
              <li><span class="year">2019 & 2020:</span> National Scholarship</li>
              <li><span class="year">2017:</span> Gold Medal of the 34th Chinese Physics Olympiad (Finals)</li>
            </ul>
          </section>

          <br>

          <h2>Selected Research</h2>
          <p>
            (* indicates equal contribution).
          </p>
          

          
          <h3>Visual Language Foundation Models</h3>

          <table
            style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr bgcolor="#ffffd0">
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="image-container">
                    <img src="images/GLM_4_1V_Thinking_thumbnail.jpg" alt="GLM-4.1V-Thinking Framework" width="250"
                    style="border-style: none">
                  </div>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2507.01006">
                    <span class="papertitle">GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning
                    </span>
                  </a>
                  <br>
                  <div class="author-list">
                      <span id="visible-authors">
                          <strong>Wenyi Hong</strong>, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, Lihang Pan, Shuaiqi Duan, Weihan Wang, Yan Wang, Yean Cheng, Zehai He, Zhe Su, Zhen Yang, Ziyang Pan, ..., Minlie Huang, Yuxiao Dong, Jie Tang
                      </span>
                      <span id="hidden-authors" class="hidden-authors">
                          , Aohan Zeng, Baoxu Wang, Boyan Shi, Changyu Pang, Chenhui Zhang, Da Yin, Fan Yang, Guoqing Chen, Jiazheng Xu, Jiali Chen, Jing Chen, Jinhao Chen, Jinghao Lin, Jinjiang Wang, Junjie Chen, Leqi Lei, Letian Gong, Leyi Pan, Mingzhi Zhang, Qinkai Zheng, Sheng Yang, Shi Zhong, Shiyu Huang, Shuyuan Zhao, Siyan Xue, Shangqin Tu, Shengbiao Meng, Tianshu Zhang, Tianwei Luo, Tianxiang Hao, Wenkai Li, Wei Jia, Xin Lyu, Xuancheng Huang, Yanling Wang, Yadong Xue, Yanfeng Wang, Yifan An, Yifan Du, Yiming Shi, Yiheng Huang, Yilin Niu, Yuan Wang, Yuanchang Yue, Yuchen Li, Yutao Zhang, Yuxuan Zhang, Zhanxiao Du, Zhenyu Hou, Zhao Xue, Zhengxiao Du, Zihan Wang, Peng Zhang, Debing Liu, Bin Xu, Juanzi Li, Minlie Huang, Yuxiao Dong, Jie Tang
                      </span>
                      <span id="author-count" class="author-count"> (77 authors) </span>
                      <button id="toggle-btn" class="toggle-button">Show all authors</button>
                  </div>
                  <p></p>
                  <a href="https://arxiv.org/abs/2507.01006">ArXiv</a>
                  /
                  <a
                    href="https://github.com/THUDM/GLM-4.1V-Thinking">GitHub</a>
                  /
                  <a href="https://huggingface.co/THUDM/GLM-4.1V-9B-Thinking">Open-sourced Models</a>
                  /
                  <a href="https://www.bigmodel.cn/dev/api/visual-reasoning-model/GLM-4.1V-Thinking">API</a>
                  /
                  <a href="data/glm_4_1v_thinking.bib">bibtex</a>
                  <p></p>
                  <p>We present <strong> GLM-4.1V-Thinking </strong>, a VLM designed to advance general-purpose multimodal understanding and reasoning. With enhanced pre-trained base and carefully optimized multi-domain RL procedure, GLM-4.1V-9B-Thinking achieves state-of-the-art performance among models of comparable size. </p>
                </td>
                  <script>
                    const toggleBtn = document.getElementById('toggle-btn');
                    const hiddenAuthors = document.getElementById('hidden-authors');
                    const authorCount = document.getElementById('author-count');
                    let isExpanded = false;

                    toggleBtn.addEventListener('click', function() {
                        if (isExpanded) {
                            // Collapse
                            hiddenAuthors.style.display = 'none';
                            authorCount.style.display = 'inline';
                            toggleBtn.textContent = 'Show all authors';
                            isExpanded = false;
                        } else {
                            // Expand
                            hiddenAuthors.style.display = 'inline';
                            authorCount.style.display = 'none';
                            toggleBtn.textContent = 'Show fewer authors';
                            isExpanded = true;
                        }
                    });
                </script>
              </tr>
              
              <tr bgcolor="#ffffd0">
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="image-container">
                    <img src="images/cogvlm2_thumbnail.jpg" alt="CogVLM2 architecture" width="250"
                    style="border-style: none">
                  </div>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2408.16500">
                    <span class="papertitle">CogVLM2: Visual Language Models for Image and Video Understanding
                    </span>
                  </a>
                  <br>
                  <strong> Wenyi Hong </strong>, Weihan Wang, Ming Ding, Wenmeng Yu, Qingsong Lv, Yan Wang, Yean Cheng,
                  Shiyu Huang, Junhui Ji, Zhao Xue, Lei Zhao, Zhuoyi Yang, Xiaotao Gu, Xiaohan Zhang, Guanyu Feng, Da
                  Yin, Zihan Wang, Ji Qi, Xixuan Song, Peng Zhang, Debing Liu, Bin Xu, Juanzi Li, Yuxiao Dong, Jie Tang
                  <p></p>
                  <a href="https://arxiv.org/pdf/2408.16500">ArXiv</a>
                  /
                  GitHub
                  (<a href="https://github.com/THUDM/CogVLM2">CogVLM2</a> & <a
                    href="https://github.com/THUDM/GLM-4">GLM-4V</a>)
                  /
                  <a href="data/cogvlm2.bib">bibtex</a>
                  <p></p>
                  <p>We propose the <strong> CogVLM2 family, a new generation of visual language models for image and video
                    understanding </strong> including CogVLM2, CogVLM2-Video and GLM-4V. </p>
                </td>
              </tr>

              <tr bgcolor="#ffffd0">
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="image-container">
                    <img src="images/CogAgent_cvpr24_thumbnail.jpg" alt="CogAgent Demo" width="250"
                    style="border-style: none">
                  </div>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2312.08914">
                    <span class="papertitle">CogAgent: A Visual Language Model for GUI Agents
                    </span>
                  </a>
                  <br>
                  <strong> Wenyi Hong </strong>, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang,
                  Zihan Wang, Yuxuan Zhang, Juanzi Li, Bin Xu, Yuxiao Dong, Ming Ding, Jie Tang
                  <br>
                  <em>CVPR</em>, 2024 <font color="red"><strong>(Highlight)</strong></font>
                  <br>
                  <a href="https://arxiv.org/pdf/2312.08914">ArXiv</a>
                  /
                  <a href="https://github.com/THUDM/CogAgent">GitHub</a>
                  /
                  <a href="https://huggingface.co/THUDM/CogAgent">Models</a>
                  /
                  <a href="https://huggingface.co/THUDM/cogagent-9b-20241220">Models (new version-241220)</a>
                  /
                  <a href="data/cogagent_cvpr24.bib">bibtex</a>
                  <p></p>
                  <p> <strong> One of first GUI agents based on pre-trained VLMs. </strong>
                    <br>
                    We introduce CogAgent, an open-sourced 18-billion-parameter visual language model (VLM) specializing
                    in GUI understanding and navigation.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="image-container">
                    <img src="images/cogvlm_neurips24_thumbnail.jpg" alt="CogVLM Demo" width="250"
                      style="border-style: none">
                  </div>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2311.03079">
                    <span class="papertitle">CogVLM: Visual Expert for Pretrained Language Models
                    </span>
                  </a>
                  <br>
                  Weihan Wang, Qingsong Lv, Wenmeng Yu, <strong> Wenyi Hong</strong>, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, Jie Tang
                  <br>
                  <em>NeurIPS</em>, 2024 
                  <br>
                  <a href="https://arxiv.org/pdf/2311.03079">ArXiv</a>
                  /
                  <a href="https://github.com/THUDM/CogVLM">GitHub</a>
                  /
                  <a href="https://huggingface.co/THUDM/cogvlm-chat-hf">Models</a>
                  /
                  <a href="data/cogvlm_neurips24.bib">bibtex</a>
                  <p></p>
                  <p>
                    We introduce CogVLM, a powerful open-source visual language foundation model. With the design of vision expert, CogVLM enables deep fusion of vision language features without sacrificing any performance on NLP tasks. 
                  </p>
                </td>
              </tr>




            </tbody>
          </table>


          <h3>Vision Generation Foundation Models</h3>

          <table
            style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr onmouseout="defocus_stop()" onmouseover="defocus_start()" bgcolor="#ffffd0">
                <!-- <td style="padding:16px;width:35%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='defocus_image'><video width=100% height=100% muted autoplay loop>
                        <source src="images/CogVideo_iclr23_lion_trim.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <div id='focus_image'>
                      <img src='images/CogVideo_iclr23_thumbnail.jpg' width="250">
                    </div>
                  </div>
                  <script type="text/javascript">
                    function defocus_start() {
                      document.getElementById('defocus_image').style.opacity = "1";
                      document.getElementById('focus_image').style.opacity = "0";
                    }

                    function defocus_stop() {
                      document.getElementById('defocus_image').style.opacity = "0";
                      document.getElementById('focus_image').style.opacity = "1";
                    }
                    defocus_stop()
                  </script>
                </td> -->
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="image-container">
                    <img src="images/CogVideo_iclr23_thumbnail.jpg" alt="CogVideo Demo" width="250"
                    style="border-style: none">
                  </div>
                </td>
                <td style="padding:8px;width:65%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2205.15868">
                    <span class="papertitle">CogVideo: Large-Scale Pretraining for Text-to-Video Generation via
                      Transformers</span>
                  </a>
                  <br>
                  <strong>Wenyi Hong</strong>*, Ming Ding*, Wendi Zheng, Xinghan Liu, Jie Tang
                  <br>
                  <em>ICLR</em>, 2023 &nbsp
                  <br>
                  <a href="https://arxiv.org/pdf/2205.15868">ArXiv</a>
                  /
                  <a href="https://github.com/THUDM/CogVideo">GitHub</a>
                  /
                  <a href="https://huggingface.co/THUDM/CogVideo">HuggingFace</a>
                  /
                  <a href="data/cogvideo_iclr23.bib">bibtex</a>
                  <p></p>
                  <p>As (probably) the <strong>first open-source large-scale pretrained text-to-video model</strong>,
                    CogVideo outperforms all publicly available models at a large margin in machine and human
                    evaluations.</p>
                </td>
              </tr>

              <tr>
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="image-container">
                    <img src="images/cogvideox_iclr25_thumbnail.jpg" alt="CogVideoX Demo" width="250"
                    style="border-style: none">
                  </div>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2408.06072">
                    <span class="papertitle">CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer
                    </span>
                  </a>
                  <br>
                  Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, <strong> Wenyi Hong </strong>, Xiaohan Zhang, Guanyu Feng, Da Yin, Yuxuan Zhang, Weihan Wang, Yean Cheng, Bin Xu, Xiaotao Gu, Yuxiao Dong, Jie Tang
                  <br>
                  <em>ICLR</em>, 2025 
                  <br>
                  <a href="https://arxiv.org/pdf/2408.06072">ArXiv</a>
                  /
                  <a href="https://github.com/THUDM/CogVideo">GitHub</a>
                  /
                  <a href="https://huggingface.co/THUDM/CogVideoX-5b">Models</a>
                  /
                  <a href="data/cogvideox_iclr25.bib">bibtex</a>
                  <p></p>
                  <p>
                    We present <strong> the 2nd generation of CogVideo -- CogVideoX </strong>, a large-scale text-to-video generation model based on diffusion transformer, which can generate 10-second continuous videos aligned with text prompt, with a frame rate of 16 fps and resolution of 768 * 1360 pixels.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="image-container">
                    <img src="images/cogview2_neurips22_thumbnail.jpg" alt="CogView2 Demo" width="250"
                    style="border-style: none">
                  </div>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2204.14217">
                    <span class="papertitle">CogView2: Faster and Better Text-to-Image Generation via Hierarchical Transformers
                    </span>
                  </a>
                  <br>
                  Ming Ding, Wendi Zheng, <strong> Wenyi Hong </strong>, Jie Tang
                  <br>
                  <em>NeurIPS</em>, 2022
                  <br>
                  <a href="https://arxiv.org/pdf/2204.14217">ArXiv</a>
                  /
                  <a href="https://github.com/THUDM/CogView2">GitHub</a>
                  /
                  <a href="data/cogview2_neurips22.bib">bibtex</a>
                  <p></p>
                  <p>
                    To boost faster and higher-resolution image generation, we propse a hierarchical transformers and local parallel auto-regressive generation, achieved by CogLM (cross-modal general language model) architecture. 
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="image-container">
                    <img src="images/cogview_neurips21_thumbnail.jpg" alt="CogView Architecture" width="250"
                    style="border-style: none">
                  </div>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2105.13290">
                    <span class="papertitle">CogView: Mastering Text-to-Image Generation via Transformers
                    </span>
                  </a>
                  <br>
                  Ming Ding, Zhuoyi Yang, <strong> Wenyi Hong </strong>, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, Jie Tang
                  <br>
                  <em>NeurIPS</em>, 2021 
                  <br>
                  <a href="https://arxiv.org/pdf/2105.13290">ArXiv</a>
                  /
                  <a href="https://github.com/THUDM/CogView">GitHub</a>
                  /
                  <a href="data/cogview_neurips21.bib">bibtex</a>
                  <p></p>
                  <p>
                    We present CogView, a 4 billion-parameter Transformer with a VQ-VAE tokenizer for general-domain text-to-image generation, developed contemporaneously with DALLÂ·E by OpenAI.
                  </p>
                </td>
              </tr>


            </tbody>
          </table>

          <h3>Evaluation of Vision Language Models</h3>

          <table
            style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr bgcolor="#ffffd0">
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="image-container">
                    <img src="images/motionbench_cvpr25_thumbnail.jpg" alt="MotionBench intro" width="250"
                      style="border-style: none">
                  </div>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2501.02955">
                    <span class="papertitle">MotionBench: Benchmarking and Improving Fine-grained Video Motion Understanding for Vision Language Models
                    </span>
                  </a>
                  <br>
                  <strong> Wenyi Hong </strong>*, Yean Cheng*, Zhuoyi Yang*, Weihan Wang, Lefan Wang, Xiaotao Gu, Shiyu Huang, Yuxiao Dong, Jie Tang
                  <br>
                  <em>CVPR</em>, 2025 
                  <br>
                  <a href="https://arxiv.org/pdf/2501.02955">ArXiv</a>
                  /
                  <a href="https://motion-bench.github.io/"> Project Page </a>
                  /
                  <a href="https://huggingface.co/datasets/THUDM/MotionBench">Dataset</a>
                  /
                  <a href="data/motionbench_cvpr25.bib">bibtex</a>
                  <p></p>
                  <p>
                    we propose MotionBench, a comprehensive evaluation benchmark designed to assess the fine-grained motion comprehension of video understanding models. Further, we propose a novel and efficient Through-Encoder (TE) Fusion method to enhance VLM's ability to perceive fine-grained motion within a limited sequence length budget. 
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="image-container">
                    <img src="images/lvbench_thumbnail.jpg" alt="LVBench intro" width="250"
                    style="border-style: none">
                  </div>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2406.08035">
                    <span class="papertitle">LVBench: An Extreme Long Video Understanding Benchmark
                    </span>
                  </a>
                  <br>
                  Weihan Wang, Zehai He, <strong> Wenyi Hong </strong> , Yean Cheng, Xiaohan Zhang, Ji Qi, Xiaotao Gu, Shiyu Huang, Bin Xu, Yuxiao Dong, Ming Ding, Jie Tang

                  <br>
                  <em>CVPR</em>, 2025 
                  <br>
                  <a href="https://arxiv.org/pdf/2406.08035">ArXiv</a>
                  /
                  <a href="https://lvbench.github.io/"> Project Page </a>
                  /
                  <a href="https://huggingface.co/datasets/THUDM/LVBench">Dataset</a>
                  /
                  <a href="data/lvbench.bib">bibtex</a>
                  <p></p>
                  <p>
                    We introduce LVBench, a benchmark specifically designed for long video understanding. Our dataset contains 6 major capability categories and 21 subcategories, with the video average length of 1.14 hours,  approximately four times longer than the longest existing dataset.
                  </p>
                </td>
              </tr>


            </tbody>
          </table>

          <h3>Vision Generation Algorithms</h3>

          <table
            style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="image-container">
                    <img src="images/infdit_eccv24_thumbnail.jpg" alt="Inf-DiT Architecture" width="250"
                    style="border-style: none">
                  </div>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2405.04312">
                    <span class="papertitle">Inf-DiT: Upsampling Any-Resolution Image with Memory-Efficient Diffusion Transformer
                    </span>
                  </a>
                  <br>
                  Zhuoyi Yang, Heyang Jiang, <strong> Wenyi Hong</strong>, Jiayan Teng, Wendi Zheng, Yuxiao Dong, Ming Ding, Jie Tang
                  <br>
                  <em>ECCV</em>, 2024 
                  <br>
                  <a href="https://arxiv.org/pdf/2405.04312">ArXiv</a>
                  /
                  <a href="https://github.com/THUDM/Inf-DiT">GitHub</a>
                  /
                  <a href="data/infdit_eccv24.bib">bibtex</a>
                  <p></p>
                  <p>
                    We propose a unidirectional block attention mechanism for image diffusion models that can adaptively adjust the memory overhead during the inference process and handle global dependencies.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="image-container">
                    <img src="images/relaydiffusion_iclr24_thumbnail.jpg" alt="Pipeline of Relay Diffusion" width="250"
                    style="border-style: none">
                  </div>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2309.03350">
                    <span class="papertitle">Relay Diffusion: Unifying diffusion process across resolutions for image synthesis
                    </span>
                  </a>
                  <br>
                  Jiayan Teng, Wendi Zheng, Ming Ding, <strong> Wenyi Hong </strong>, Jianqiao Wangni, Zhuoyi Yang, Jie Tang
                  <br>
                  <em>ICLR</em>, 2024 <font color="red"><strong>(Spotlight)</strong></font>
                  <br>
                  <a href="https://arxiv.org/pdf/2309.03350">ArXiv</a>
                  /
                  <a href="https://github.com/THUDM/RelayDiffusion">GitHub</a>
                  /
                  <a href="data/relaydiffusion_iclr24.bib">bibtex</a>
                  <p></p>
                  <p>
                    Through the lens of discrete cosine transformation, we find the main reason for difficulty in high-resolution generation with diffusion models is that the same noise level on a higher resolution results in a higher Signal-to-Noise Ratio in the frequency domain. In this work, we present Relay Diffusion Model (RDM), where the diffusion process can continue seamlessly in any new resolution or model without restarting from pure noise or low-resolution conditioning.
                  </p>
                </td>
              </tr>


            </tbody>
          </table>



          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:center;font-size:small;">
                    Reference code: <a href="https://github.com/jonbarron/jonbarron_website">source
                      code</a>.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
  </table>
</body>

</html>